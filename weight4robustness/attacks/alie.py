import torch
import numpy as np
from scipy.stats import norm
from .attack import Attack


class ALittleIsEnoughAttack(Attack):
    """
    Implementation of the 'A Little Is Enough' attack strategy for Byzantine-robust distributed learning.
    This attack perturbs updates based on the weighted standard deviation of honest updates.
    """

    def __init__(self):
        """
        Initializes the ALittleIsEnoughAttack class.
        """
        super().__init__()

    @staticmethod
    def weighted_std(values, weights, dim=0):
        """
        Computes the weighted standard deviation along the specified dimension.

        Args:
            values (torch.Tensor): The input tensor of values.
            weights (torch.Tensor): The tensor of weights corresponding to each value.
            dim (int): The dimension along which the weighted standard deviation is computed.

        Returns:
            torch.Tensor: The weighted standard deviation.
        """
        # Compute weighted mean
        weighted_mean = torch.sum(values * weights, dim=dim) / torch.sum(weights, dim=dim)
        weighted_mean_expanded = weighted_mean.unsqueeze(dim)

        # Compute weighted variance
        weighted_variance = torch.sum(weights * (values - weighted_mean_expanded) ** 2, dim=dim) / torch.sum(weights,
                                                                                                             dim=dim)

        # Return the weighted standard deviation
        return torch.sqrt(weighted_variance)

    def apply(self, honest_updates, weights, *args, **kwargs):
        """
        Applies the 'A Little Is Enough' attack by perturbing the aggregated updates based on the
        weighted standard deviation of honest updates.

        Args:
            honest_updates (torch.Tensor): Aggregated honest updates from workers.
            weights (torch.Tensor): Total weights across all iterations.

        Returns:
            torch.Tensor: Perturbed updates generated by the attack.
        """
        # Calculate total and Byzantine iterations
        honest_num = honest_updates.shape[0]
        honest_weights = weights[:honest_num]
        total_iterations = weights.sum().item()
        byzantine_iterations = total_iterations - honest_weights.sum().item()

        # Compute critical values for attack
        s = np.floor(total_iterations / 2 + 1) - byzantine_iterations
        cdf_value = (total_iterations - byzantine_iterations - s) / (total_iterations - byzantine_iterations)
        z_max = norm.ppf(cdf_value)

        # Compute mean and standard deviation of honest updates
        mu = torch.sum((honest_weights / honest_weights.sum()) * honest_updates, dim=0)
        std = self.weighted_std(honest_updates, honest_weights)

        # Return perturbed update
        return mu - std * z_max

