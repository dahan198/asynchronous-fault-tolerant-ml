from .attack import Attack


class LabelFlippingAttack(Attack):
    """
    Implements the Label Flipping Attack, a simple but effective Byzantine attack strategy
    where the labels are inverted before computing the gradient. This can significantly
    degrade the performance of machine learning models in distributed learning setups.
    """

    def __init__(self):
        """
        Initializes the LabelFlippingAttack class.

        """
        super().__init__()

    def apply(self, inputs, targets, worker, gradient_function, *args, **kwargs):
        """
        Applies the label flipping attack by inverting the labels and computing the corresponding gradient.

        Args:
            inputs (torch.Tensor): Input data.
            targets (torch.Tensor): True target labels.
            worker (Worker): The worker applying the attack.
            gradient_function (callable): Function to compute the gradient given inputs and labels.

        Returns:
            torch.Tensor: Perturbed update generated by applying the label flipping attack.
        """
        # Invert the labels (for MNIST-like datasets, flipping 0-9 to 9-0)
        flipped_targets = 9 - targets

        # Compute the gradient with flipped labels
        gradient, _, _ = gradient_function(inputs, flipped_targets)

        # Return the worker's updated step with the perturbed gradient
        return worker.step(gradient)
